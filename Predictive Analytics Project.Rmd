---
title: "Predictive Analytics Project"
author: "Rachel Jordan"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Appendix

## Load data

```{r}
#load data
test <- read.csv("test space titanic.csv")
train <- read.csv("train space titanic.csv")
```

## Wrangle data and impute

```{r}
#load tidyverse
require(tidyverse)

#create group, number in group, deck, num, side variables
train <- train %>% 
  mutate(Group = substr(PassengerId, start = 1, stop = 4)) %>% 
  mutate(NumberinGroup = substr(PassengerId, start = 6, stop = 7)) %>% 
  separate(Cabin, into=c("Deck","Num","Side"), sep="/")

#change to factor and numeric.
train <- as.data.frame(unclass(train), stringsAsFactors = TRUE)
train$Num <- as.numeric(train$Num)
train$Age <- as.numeric(train$Age)
train$RoomService <- as.numeric(train$RoomService)
train$FoodCourt <- as.numeric(train$FoodCourt)
train$ShoppingMall <- as.numeric(train$ShoppingMall)
train$Spa <- as.numeric(train$Spa)
train$VRDeck <- as.numeric(train$VRDeck)
train$Group <- as.numeric(train$Group)
train$NumberinGroup <- as.numeric(train$NumberinGroup)

#drop passenger name and passengerid
train <- train %>% 
  select(-c(PassengerId,Name))

#impute
require(mice)

train <- train %>% 
  mutate_all(na_if,"")

set.seed(1900)
imputed_data <- mice(data = train, m = 5, printFlag = F)

train_imputed <- complete(imputed_data, 1)

#function to sum missing values
missing <- function(x) {
  sum(is.na(x))
}

apply(train_imputed, 2, missing)

#and now I actually have to split this into a sub train and sub test because we don't have the response for the test data
set.seed(98393)
train_sample <- sample(1:8693,8693*0.8,replace=F)
training <- train_imputed[train_sample,]
testing <- train_imputed[-train_sample,]

#write to csv for use in google colab
write.csv(training,"training export.csv")
write.csv(testing, "testing export.csv")
```

## EDA

```{r}
#break out numeric variables
numeric_preds <- training %>% 
  select(Num, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, Group, NumberinGroup)

#correlation matrix
cor(numeric_preds) #pretty low correlations

#look at money spending to see if there's any pattern there
money_totals <- training %>% 
  mutate(total_spent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck)

#visualize
ggplot(money_totals, mapping = aes(y = total_spent)) + geom_boxplot() + facet_grid(cols = vars(Transported)) + coord_cartesian(ylim = c(0,5000)) #people who were transported tended to spend less money overall than people who weren't

#look at side of the ship
require(vcd)
sides <- xtabs(~Side + Transported, data = training)
sides_stdres <- chisq.test(sides)$stdres
mosaic(sides, gp = shading_Friendly, residuals = sides_stdres,
       residuals_type = "Std/nresiduals", labeling = labeling_residuals()) #more S were transported, less P were transported
chisq.test(sides) #this p-value is sigificant, side is not independent of transportation

#look at deck
ggplot(training, mapping = aes(x = Deck)) + geom_bar(aes(fill = Transported), position = "dodge") #looks like B and C decks were more likely to get transported and E and F were less likely to get transported

#look at Num
ggplot(training, mapping = aes(y = Num)) + geom_boxplot() + facet_grid(cols = vars(Transported)) #this doesn't seem to really matter

#look at Age
ggplot(training, mapping = aes(y = Age)) + geom_boxplot() + facet_grid(cols = vars(Transported)) #this doesn't seem to really matter either

#look at destination
xtabs(~Destination + Transported, data = training)
#seems like a lot more cancri people were transported than not - doesn't seem to be much of a difference for the others

#look at homeplanet
xtabs(~HomePlanet + Transported, data = training)
#lotta europa people got transported, looks like more earth people did not, mars seems neutral

#look at cryosleep
xtabs(~CryoSleep + Transported, data = training)
#this seems to really matter - cryosleep seams to lead to transportation
```

## Trying logistic regression

```{r}
#fit model
log_reg <- glm(Transported ~ ., data=training, family="binomial")
#warning: fitted probabiliites numerically 0 or 1 occurred

#calculate error rate
log_reg_preds <- ifelse(predict(log_reg, newdata = testing, type="response") > 0.5,"True","False")

#error rate
table(log_reg_preds,testing$Transported)
(173 + 200)/nrow(testing) #21.4% which is honestly not too shabby. 
```

## Try Support Vector Classifier and Machine

```{r}
#support vector classifier
require(e1071)
set.seed(19)
tune_out_svc <- tune(svm, Transported~.,data=training, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_out_svc)
#we want cost to be 5
set.seed(8383)
best_svc <- svm(Transported ~ ., data = training, kernel = "linear", cost=5)
summary(best_svc)

#calculate error rate
table(predict(best_svc,testing),testing$Transported)
(233+137)/nrow(testing) #21.3% error, barely better than the logistic regression model which tracks based on the homework question I just completed

#support vector machine with polynomial kernel
set.seed(99)
tune_out_svm_poly <- tune(svm, Transported ~ ., data=training, kernel= "polynomial", ranges = list(
  cost = c(0.001, 1, 10, 100)
)) #10 is best. i can't tune cost and gamma at the same time because it runs for ages and ages and ages
summary(tune_out_svm_poly)

set.seed(99)
tune_out_svm_poly_2 <- tune(svm, Transported ~ ., data=training, kernel= "polynomial", ranges = list(
  cost = c(1, 5, 10, 50, 100)
)) #50 is best
summary(tune_out_svm_poly_2)

set.seed(99)
tune_out_svm_poly_3 <- tune(svm, Transported ~ ., data=training, kernel= "polynomial", ranges = list(
  cost = c(30, 40, 50, 60, 70)
)) #40 is best
summary(tune_out_svm_poly_3)

set.seed(2838)
svm_poly <- svm(Transported ~ . , data=training, kernel = "polynomial", cost = 40, degree=2)
poly_preds <- predict(svm_poly,testing)
table(poly_preds,testing$Transported)
(175+165)/nrow(testing) #19.6% error rate

#mess around with degree
set.seed(2838)
svm_poly_2 <- svm(Transported ~ . , data=training, kernel = "polynomial", cost = 40, degree=2)
poly_preds_2 <- predict(svm_poly_2,testing)
table(poly_preds_2,testing$Transported)
(151+205)/nrow(testing) #19.6% error rate -- through manual testing I have determined that 2 is the best degree for this
#my computer does NOT want me to tune gamma i am very sorry

#support vector machine with radial kernel
set.seed(283778)
tune_out_svm_radial <- tune(svm, Transported ~ . , data=training, kernel = "radial", ranges = list(
  cost = c(0.1,1,10,100,1000)
)) #use cost = 10
summary(tune_out_svm_radial)

set.seed(0604)
svm_radial <- svm(Transported ~ . , data=training, kernel = "radial", cost = 10)
radial_preds <- predict(svm_radial,testing)
table(radial_preds,testing$Transported)
(179 + 150)/nrow(testing) #18.9% error rate
#ok this is pretty good actually, I'm going to tune this more

set.seed(28378)
tune_out_svm_radial_2 <- tune(svm, Transported ~ . , data=training, kernel = "radial", ranges = list(
  cost = c(5,7,9,11,13)
)) #use cost = 10
summary(tune_out_svm_radial_2)

set.seed(0604)
svm_radial_2 <- svm(Transported ~ . , data=training, kernel = "radial", cost = 13)
radial_preds_2 <- predict(svm_radial_2,testing)
table(radial_preds_2,testing$Transported)
(173 + 148)/nrow(testing) #18.5% error rate

#i tuned gamma a little but it didn't improve the model at all
```

## Random Forest and Bagging

```{r}
#load randomforest package
require(randomForest)

#try bagging
set.seed(8488)
bag_model <- randomForest(Transported ~ ., data = training, mtry = 15, importance = T)
bag_model #OOB estimate of error rate = 20.61%

#try randomForest
set.seed(8484949) 
rf_model <- randomForest(Transported ~ ., data = training, importance = T)
rf_model #OOB estimate of error rate = 19%

#experiment with number of trees and mtry
find_ntree <- function(x) {
  set.seed(19)
  ntree_result <- randomForest(Transported ~ ., data = training, ntree = x)
 return(mean(ntree_result$confusion[,3]))
}

possible_x <- c(1:500)
ntree_stuff <- sapply(possible_x,find_ntree)
which.min(ntree_stuff) #use ntree = 296

#visualize
viz_ntree <- data.frame(possible_x,ntree_stuff)
ggplot(viz_ntree, aes(x = possible_x, y = ntree_stuff)) + geom_line() + xlab("Possible Ntree Values") + ylab("Classification Error Rate")

find_mtry <- function(x) {
  set.seed(19)
  mtry_result <- randomForest(Transported ~ ., data = training, ntree = 296, mtry = x)
  return(mean(mtry_result$confusion[,3]))
}

possible_mtry <- c(1:15)
mtry_stuff <- sapply(possible_mtry, find_mtry)
which.min(mtry_stuff) #use mtry = 3

#visualize
viz_mtry <- data.frame(possible_mtry,mtry_stuff)
ggplot(viz_mtry, aes(x = possible_mtry, y = mtry_stuff)) + geom_line() + xlab("Possible MTry Values") + ylab("Classification Error Rate") 

set.seed(19)
best_rf <- randomForest(Transported ~ ., data = training, ntree = 296, mtry = 3)
best_rf #OOB error rate 18.91%

#cross-validate on test set
set.seed(19)
rf_preds <- predict(best_rf, newdata = testing)
table(rf_preds, testing$Transported)
(181 + 157)/nrow(testing) #19.4% error rate
```

## Try LASSO

```{r}
#convert to model matrix
model_matrix_train <- model.matrix(Transported ~ ., data = training)[,-1]
response <- training %>% 
  mutate(Binary = case_when(
    Transported == "True" ~ 1,
    Transported == "False" ~ 0
  ))
response <- response$Binary

model_matrix_test <- model.matrix(Transported ~ ., data = testing)[,-1]
response_test <- testing %>% 
  mutate(Binary = case_when(
    Transported == "True" ~ 1,
    Transported == "False" ~ 0
  ))
response_test <- response_test$Binary

#lasso
require(glmnet)
set.seed(83838)
grid <- 10^seq(10,-2,length=100)
ridge_mod <- glmnet(model_matrix_train, response, alpha = 1, lambda = grid)
ridge_cv <- cv.glmnet(model_matrix_train, response, alpha = 1)
bestlam_ridge <- ridge_cv$lambda.min
ridge_pred <- predict(ridge_mod, s=bestlam_ridge, newx = model_matrix_test, type = "coefficients")
ridge_pred
#this indicates that we can try dropping VIP and number in group in other models because it reduces them to 0
```

## Try SVM and Random Forest on a Reduced Model

```{r}
#random forest
set.seed(19)
best_rf_2 <- randomForest(Transported ~ HomePlanet + CryoSleep + Deck + Num + Side + Destination + Age + RoomService + FoodCourt + ShoppingMall + Spa + VRDeck + Group, data = training, ntree = 296, mtry = 3)
best_rf_2 #OOB error rate 18.91%

#cross-validate on test set
set.seed(19)
rf_preds_2 <- predict(best_rf_2, newdata = testing)
table(rf_preds_2, testing$Transported)
(176 + 150)/nrow(testing) #18.7% error rate

#SVM
set.seed(0604)
svm_radial_3 <- svm(Transported ~ HomePlanet + CryoSleep + Deck + Num + Side + Destination + Age + RoomService + FoodCourt + ShoppingMall + Spa + VRDeck + Group, data=training, kernel = "radial", cost = 13)
radial_preds_3 <- predict(svm_radial_3,testing)
table(radial_preds_3,testing$Transported)
(173 + 157)/nrow(testing) #19.0% error rate
```

## Prepare Kaggle Submission CSV

```{r}
#prepare test data
#create group, number in group, deck, num, side variables
test <- test %>% 
  mutate(Group = substr(PassengerId, start = 1, stop = 4)) %>% 
  mutate(NumberinGroup = substr(PassengerId, start = 6, stop = 7)) %>% 
  separate(Cabin, into=c("Deck","Num","Side"), sep="/")

#change to factor and numeric.
test <- as.data.frame(unclass(test), stringsAsFactors = TRUE)
test$Num <- as.numeric(test$Num)
test$Age <- as.numeric(test$Age)
test$RoomService <- as.numeric(test$RoomService)
test$FoodCourt <- as.numeric(test$FoodCourt)
test$ShoppingMall <- as.numeric(test$ShoppingMall)
test$Spa <- as.numeric(test$Spa)
test$VRDeck <- as.numeric(test$VRDeck)
test$Group <- as.numeric(test$Group)
test$NumberinGroup <- as.numeric(test$NumberinGroup)

#drop passenger name and passengerid
test <- test %>% 
  select(-c(PassengerId,Name))

#impute

test <- test %>% 
  mutate_all(na_if,"")

set.seed(1900)
imputed_data_test <- mice(data = test, m = 5, printFlag = F) 

test_imputed <- complete(imputed_data_test,1)
test_reload <- read.csv("test space titanic.csv")
test_final <- cbind(test_reload$PassengerId,test_imputed)

final_predictions <- predict(svm_radial_2,test_final)
kagle_submission <- data.frame(test_final$`test_reload$PassengerId`,final_predictions)
colnames(kagle_submission) <- c("PassengerId","Transported")
kagle_submission <- kagle_submission %>% 
  mutate(Transported2 = as.character(Transported)) %>% 
  select(PassengerId,Transported2)
colnames(kagle_submission) <- c("PassengerId","Transported")
write.csv(kagle_submission,"Kagle_Submission_Final.csv",row.names = F)
Kagle_Submission_Final <- read_csv("Kagle_Submission_Final.csv", col_types = cols(Transported = col_character()))
write.csv(Kagle_Submission_Final,"Final_Kaggle_Submission.csv")

trying_again <- predict(best_rf_2, newdata = test_final)
kaggle_submission <- data.frame(test_final$`test_reload$PassengerId`,trying_again)
colnames(kaggle_submission) <- c("PassengerId","Transported")
write.csv(kaggle_submission,"Second Kaggle Submission.csv",row.names = F)
#i got position 1502 on the kaggle leaderboard with a score of 0.79214
```
